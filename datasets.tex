
Experiments are performed on the male subsets of the 2004, 2005, 2006 and 2008 NIST SRE datasets, which are from here on referred to as NIST'0x.  The NIST data is of telephony quality, with a sampling frequency of 8~kHz and $\mu$-law compression.
The NIST'04 and NIST'08 datasets are used for UBM training.
The NIST'05 dataset is used for development, whereas the NIST'06 dataset is used for evaluation;  
only results for the latter are reported here.
Due to the significant amount of data necessary to estimate the total variability matrix $T$ used with the IV-PLDA system, the NIST'06 dataset is added to the pool of background data for development whereas the NIST'05 dataset was used for evaluation. 
$T$ is thus learned using approximately 11,000 utterances from 900 speakers, while independence between development and evaluation datasets is always respected.
 
So as to provide suitable training or adaptation data for speech synthesis and voice conversion spoofing attacks, and for consistency with our previous work, all experiments relate to the 8conv4w-1conv4w condition, where one conversation corresponds to an average of 2.5~minutes of speech (one side of a 5 minute conversation).
In all cases, however, only one of the eight, randomly selected training conversations is used for enrolment. 
Results presented in this paper are thus comparable to those in the literature (not work related to spoofing) for the 1conv4w-1conv4w condition. 

<<<<<<< .mine
Standard NIST protocols dictate 1,352 true client tests and 13,170 impostor tests for development and evaluation datasets. 
To assess replay spoofing, impostor tests are replaced with spoofed versions, whereas genuine client trials are unchanged.
In the case of replay , this approach naturally reduces the number of feasible attacks; whereas zero-effort impostor trials, speech synthesis and voice conversion attacks can involve any number of impostors, replay spoofing attacks are formulated for a single target speaker, namely that whose model is under attack.  As a result, while all experiments related to replay spoofing or countermeasure assessment involve the same true client tests as for the baseline condition, there is a reduced number of 1,352 spoofed trials.  Given the constraints, the latter is unavoidable whereas speech synthesis and voice conversion attacks still involve the full 13,170 spoofing trials.  This setup broadly conforms to the general protocols outlined in~\cite{Wu2014a,Wu2015} and the general evaluation methodology described in~\cite{Hadid2015}.
=======
Standard NIST protocols dictate 1,352 true client tests and 12,648 impostor tests for development and evaluation datasets. 
To assess spoofing, impostor tests are replaced with spoofed versions of the original utterance, whereas genuine client trials are unchanged.
This approach naturally reduces the number of feasible impostor/spoof trials; whereas zero-effort impostor trials can involve any number of impostors, spoofing attacks are formulated for a single target speaker, namely that whose model is under attack.  As a result, while all experiments related to spoofing or countermeasure assessment involve the same true client tests as for the baseline condition, there is a reduced number of approximately 1,000 spoofed trials.  Given the constraints, the latter is unavoidable.  This setup broadly conforms to the general protocols outlined in~\cite{Wu2014a,Wu2015} and the general evaluation methodology described in~\cite{Hadid2015}.
>>>>>>> .r63

%and  the spoofing condition, there are spoofing trials their number is limited 
%When testing countermeasures independently from the ASV system, all available genuine and spoof trials are used, similarly to the protocol used in~\cite{Wu2015}}.

Other, unavoidable differences between the baseline and spoofing protocols relate to the variability in phone content.  Voice conversion attacks naturally have the same phone content as the original zero-effort impostor trials.  Speech synthesis attacks were generated using arbitrary texts with a similar duration to that of the original zero-effort imposter trial.  Replay attacks naturally contain the same phone content as that in another recording of the same, client speaker.  Since differences in phone content are perhaps undesirable, the use of relatively long duration trials ensures that they have negligible impact on results; the use of long duration trials ensures that any effect of phone variation is essentially normalised.  


Given the consideration of spoofing, and without any standard operating criteria under such a scenario, the EER metric is preferred to the minimum detection cost function (minDCF).  Also reported is the spoofing false acceptance rate (SFAR,~\cite{Johnson2010}) for a false rejection rate (FRR) fixed to the baseline EER.
