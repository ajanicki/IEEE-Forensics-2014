
Experiments are performed on the male subsets of the 2004, 2005, 2006 and 2008 NIST SRE datasets, which are from here on referred to as NIST'0x.  The NIST data is of telephony quality, with a sampling frequency of 8~kHz and $\mu$-law compression.
The NIST'04 and NIST'08 datasets are used for UBM training.
The NIST'05 dataset is used for development, whereas the NIST'06 dataset is used for evaluation;  
only results for the latter are reported here.
Due to the significant amount of data necessary to estimate the total variability matrix $T$ used with the IV-PLDA system, the NIST'06 dataset is added to the pool of background data for development whereas the NIST'05 dataset was used for evaluation. 
$T$ is thus learned using approximately 11,000 utterances from 900 speakers, while independence between development and evaluation datasets is always respected.
 
So as to provide suitable training or adaptation data for speech synthesis and voice conversion spoofing attacks, and for consistency with our previous work, all experiments relate to the 8conv4w-1conv4w condition, where one conversation corresponds to an average of 2.5~minutes of speech (one side of a 5 minute conversation).
In all cases, however, only one of the eight, randomly selected training conversations is used for enrolment. 
Results presented in this paper are thus comparable to those in the literature (not work related to spoofing) for the 1conv4w-1conv4w condition. 
Standard NIST protocols dictate in the order of 1,000 true client tests and 10,000 impostor tests for development and evaluation datasets. 
To assess spoofing, impostor tests are replaced with spoofed versions of the original utterance, whereas genuine client trials are unchanged. {\bfseries Even though differences in phonetic content between individual NIST recordings are inevitable, they are effectively normalised considering their relatively long duration.}
This setup conforms to the general protocols outlined in~\cite{Wu2014a} and the broader evaluation methodology described in~\cite{Hadid2015}. {\bfseries When testing countermeasures independently from the ASV system, all available genuine and spoof trials are used, similarly to the protocol used in~\cite{Wu2015}}.

Given the consideration of spoofing, and without any standard operating criteria under such a scenario, the EER metric is preferred to the minimum detection cost function (minDCF).  Also reported is the spoofing false acceptance rate (SFAR,~\cite{Johnson2010}) for a false rejection rate (FRR) fixed to the baseline EER.
